{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3d90642-6ceb-4998-9f4b-0baf64c4d0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in c:\\users\\utkar\\anaconda3\\lib\\site-packages (8.3.32)\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from ultralytics) (3.9.2)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from ultralytics) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from ultralytics) (10.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from ultralytics) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from ultralytics) (2.5.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from ultralytics) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from ultralytics) (4.66.5)\n",
      "Requirement already satisfied: psutil in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from ultralytics) (5.9.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from ultralytics) (2.2.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from ultralytics) (2.0.11)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
      "Requirement already satisfied: filelock in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7276f1a1-836e-408c-b429-7e52ef46c9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflowlite (from versions: none)\n",
      "ERROR: No matching distribution found for tensorflowlite\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baf9bc57-9026-44a1-a210-6641e0dde5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting supervision\n",
      "  Downloading supervision-0.25.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.7 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from supervision) (1.2.0)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from supervision) (0.7.1)\n",
      "Requirement already satisfied: matplotlib>=3.6.0 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from supervision) (3.9.2)\n",
      "Requirement already satisfied: numpy>=1.23.3 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from supervision) (1.26.4)\n",
      "Requirement already satisfied: opencv-python>=4.5.5.64 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from supervision) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=9.4 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from supervision) (10.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from supervision) (6.0.1)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from supervision) (1.13.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from matplotlib>=3.6.0->supervision) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from matplotlib>=3.6.0->supervision) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from matplotlib>=3.6.0->supervision) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from matplotlib>=3.6.0->supervision) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from matplotlib>=3.6.0->supervision) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from matplotlib>=3.6.0->supervision) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->supervision) (1.16.0)\n",
      "Downloading supervision-0.25.0-py3-none-any.whl (181 kB)\n",
      "Installing collected packages: supervision\n",
      "Successfully installed supervision-0.25.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc89387-e436-4ca3-badd-85312dfec834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f37a933-2732-4007-8c7a-7601e74037a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection and tracking started... Press 'q' to quit\n",
      "\n",
      "0: 480x640 2 persons, 1 remote, 498.3ms\n",
      "Speed: 22.8ms preprocess, 498.3ms inference, 27.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 194.6ms\n",
      "Speed: 9.7ms preprocess, 194.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 188.0ms\n",
      "Speed: 9.2ms preprocess, 188.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 200.0ms\n",
      "Speed: 4.0ms preprocess, 200.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 218.0ms\n",
      "Speed: 5.0ms preprocess, 218.0ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 235.8ms\n",
      "Speed: 6.0ms preprocess, 235.8ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 273.0ms\n",
      "Speed: 4.0ms preprocess, 273.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 212.2ms\n",
      "Speed: 5.0ms preprocess, 212.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 228.0ms\n",
      "Speed: 3.5ms preprocess, 228.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 163.4ms\n",
      "Speed: 5.0ms preprocess, 163.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 158.6ms\n",
      "Speed: 5.8ms preprocess, 158.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 167.9ms\n",
      "Speed: 4.0ms preprocess, 167.9ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 147.5ms\n",
      "Speed: 4.2ms preprocess, 147.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 174.4ms\n",
      "Speed: 5.2ms preprocess, 174.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 145.9ms\n",
      "Speed: 4.0ms preprocess, 145.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 160.5ms\n",
      "Speed: 3.4ms preprocess, 160.5ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 177.3ms\n",
      "Speed: 4.8ms preprocess, 177.3ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 188.5ms\n",
      "Speed: 4.0ms preprocess, 188.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 174.4ms\n",
      "Speed: 4.4ms preprocess, 174.4ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 255.6ms\n",
      "Speed: 4.1ms preprocess, 255.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 323.3ms\n",
      "Speed: 4.0ms preprocess, 323.3ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 243.4ms\n",
      "Speed: 4.4ms preprocess, 243.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 151.7ms\n",
      "Speed: 5.4ms preprocess, 151.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 158.9ms\n",
      "Speed: 4.1ms preprocess, 158.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 143.7ms\n",
      "Speed: 4.6ms preprocess, 143.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 151.7ms\n",
      "Speed: 3.0ms preprocess, 151.7ms inference, 5.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 164.8ms\n",
      "Speed: 4.0ms preprocess, 164.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 135.7ms\n",
      "Speed: 3.8ms preprocess, 135.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 171.4ms\n",
      "Speed: 4.3ms preprocess, 171.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 148.7ms\n",
      "Speed: 4.0ms preprocess, 148.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 146.1ms\n",
      "Speed: 4.4ms preprocess, 146.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 150.2ms\n",
      "Speed: 4.7ms preprocess, 150.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 147.7ms\n",
      "Speed: 3.0ms preprocess, 147.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 184.8ms\n",
      "Speed: 5.0ms preprocess, 184.8ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 169.8ms\n",
      "Speed: 5.3ms preprocess, 169.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 147.3ms\n",
      "Speed: 5.5ms preprocess, 147.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 139.9ms\n",
      "Speed: 3.0ms preprocess, 139.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 120.3ms\n",
      "Speed: 3.2ms preprocess, 120.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 118.1ms\n",
      "Speed: 4.4ms preprocess, 118.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 116.6ms\n",
      "Speed: 3.6ms preprocess, 116.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 117.4ms\n",
      "Speed: 3.4ms preprocess, 117.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 115.1ms\n",
      "Speed: 3.3ms preprocess, 115.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 235.0ms\n",
      "Speed: 4.5ms preprocess, 235.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 215.9ms\n",
      "Speed: 4.0ms preprocess, 215.9ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 229.6ms\n",
      "Speed: 4.0ms preprocess, 229.6ms inference, 4.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 247.2ms\n",
      "Speed: 4.4ms preprocess, 247.2ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 180.6ms\n",
      "Speed: 4.4ms preprocess, 180.6ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 124.9ms\n",
      "Speed: 4.0ms preprocess, 124.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 128.5ms\n",
      "Speed: 3.1ms preprocess, 128.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 165.0ms\n",
      "Speed: 4.8ms preprocess, 165.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 136.2ms\n",
      "Speed: 3.3ms preprocess, 136.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 116.6ms\n",
      "Speed: 3.1ms preprocess, 116.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 109.7ms\n",
      "Speed: 3.6ms preprocess, 109.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 123.5ms\n",
      "Speed: 3.3ms preprocess, 123.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 135.9ms\n",
      "Speed: 4.9ms preprocess, 135.9ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 152.1ms\n",
      "Speed: 6.4ms preprocess, 152.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 133.3ms\n",
      "Speed: 3.4ms preprocess, 133.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 127.9ms\n",
      "Speed: 4.0ms preprocess, 127.9ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 153.1ms\n",
      "Speed: 4.1ms preprocess, 153.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 155.7ms\n",
      "Speed: 3.8ms preprocess, 155.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 152.8ms\n",
      "Speed: 3.7ms preprocess, 152.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 152.1ms\n",
      "Speed: 7.2ms preprocess, 152.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 140.1ms\n",
      "Speed: 3.0ms preprocess, 140.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 135.2ms\n",
      "Speed: 3.4ms preprocess, 135.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 127.9ms\n",
      "Speed: 3.8ms preprocess, 127.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 135.9ms\n",
      "Speed: 4.1ms preprocess, 135.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 173.4ms\n",
      "Speed: 4.2ms preprocess, 173.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 209.1ms\n",
      "Speed: 3.7ms preprocess, 209.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 143.7ms\n",
      "Speed: 3.2ms preprocess, 143.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 144.1ms\n",
      "Speed: 3.8ms preprocess, 144.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 144.2ms\n",
      "Speed: 3.5ms preprocess, 144.2ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 131.8ms\n",
      "Speed: 4.1ms preprocess, 131.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 117.8ms\n",
      "Speed: 3.3ms preprocess, 117.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 117.2ms\n",
      "Speed: 4.1ms preprocess, 117.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 129.6ms\n",
      "Speed: 3.6ms preprocess, 129.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 137.1ms\n",
      "Speed: 5.4ms preprocess, 137.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 123.6ms\n",
      "Speed: 3.1ms preprocess, 123.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 116.9ms\n",
      "Speed: 3.1ms preprocess, 116.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 115.7ms\n",
      "Speed: 3.4ms preprocess, 115.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 118.6ms\n",
      "Speed: 3.9ms preprocess, 118.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 114.1ms\n",
      "Speed: 4.0ms preprocess, 114.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 115.7ms\n",
      "Speed: 4.4ms preprocess, 115.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 110.2ms\n",
      "Speed: 4.2ms preprocess, 110.2ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 116.3ms\n",
      "Speed: 2.6ms preprocess, 116.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 116.6ms\n",
      "Speed: 4.1ms preprocess, 116.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 115.1ms\n",
      "Speed: 3.8ms preprocess, 115.1ms inference, 9.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 126.2ms\n",
      "Speed: 4.0ms preprocess, 126.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 handbag, 1 tie, 125.8ms\n",
      "Speed: 4.0ms preprocess, 125.8ms inference, 9.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 handbag, 1 tie, 118.0ms\n",
      "Speed: 3.0ms preprocess, 118.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 121.5ms\n",
      "Speed: 3.0ms preprocess, 121.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 handbag, 1 tie, 122.7ms\n",
      "Speed: 3.0ms preprocess, 122.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 196.7ms\n",
      "Speed: 3.4ms preprocess, 196.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 123.2ms\n",
      "Speed: 3.6ms preprocess, 123.2ms inference, 4.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 115.8ms\n",
      "Speed: 3.6ms preprocess, 115.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 117.6ms\n",
      "Speed: 3.7ms preprocess, 117.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 163.1ms\n",
      "Speed: 4.0ms preprocess, 163.1ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 138.8ms\n",
      "Speed: 3.7ms preprocess, 138.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 131.3ms\n",
      "Speed: 2.8ms preprocess, 131.3ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 116.9ms\n",
      "Speed: 3.5ms preprocess, 116.9ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 140.4ms\n",
      "Speed: 4.1ms preprocess, 140.4ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 128.0ms\n",
      "Speed: 4.0ms preprocess, 128.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.6ms\n",
      "Speed: 3.0ms preprocess, 134.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 141.0ms\n",
      "Speed: 4.1ms preprocess, 141.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 126.9ms\n",
      "Speed: 4.0ms preprocess, 126.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 135.9ms\n",
      "Speed: 3.6ms preprocess, 135.9ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 149.9ms\n",
      "Speed: 4.1ms preprocess, 149.9ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 223.8ms\n",
      "Speed: 5.0ms preprocess, 223.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 188.9ms\n",
      "Speed: 3.5ms preprocess, 188.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 167.2ms\n",
      "Speed: 3.0ms preprocess, 167.2ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 145.9ms\n",
      "Speed: 4.0ms preprocess, 145.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 145.5ms\n",
      "Speed: 4.1ms preprocess, 145.5ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 154.1ms\n",
      "Speed: 8.4ms preprocess, 154.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 wine glass, 136.3ms\n",
      "Speed: 3.9ms preprocess, 136.3ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 144.7ms\n",
      "Speed: 4.2ms preprocess, 144.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 131.9ms\n",
      "Speed: 4.0ms preprocess, 131.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 123.7ms\n",
      "Speed: 4.1ms preprocess, 123.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 124.9ms\n",
      "Speed: 3.0ms preprocess, 124.9ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 136.3ms\n",
      "Speed: 4.0ms preprocess, 136.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 154.0ms\n",
      "Speed: 2.8ms preprocess, 154.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 270.3ms\n",
      "Speed: 4.5ms preprocess, 270.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 215.0ms\n",
      "Speed: 5.0ms preprocess, 215.0ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 186.4ms\n",
      "Speed: 5.0ms preprocess, 186.4ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 ties, 127.1ms\n",
      "Speed: 3.3ms preprocess, 127.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 134.7ms\n",
      "Speed: 4.0ms preprocess, 134.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 126.7ms\n",
      "Speed: 4.0ms preprocess, 126.7ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 142.0ms\n",
      "Speed: 4.3ms preprocess, 142.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 128.2ms\n",
      "Speed: 3.4ms preprocess, 128.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 127.0ms\n",
      "Speed: 4.1ms preprocess, 127.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 126.0ms\n",
      "Speed: 3.7ms preprocess, 126.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 124.6ms\n",
      "Speed: 4.0ms preprocess, 124.6ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 134.1ms\n",
      "Speed: 4.0ms preprocess, 134.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 133.9ms\n",
      "Speed: 2.8ms preprocess, 133.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 135.1ms\n",
      "Speed: 4.1ms preprocess, 135.1ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 toothbrush, 228.6ms\n",
      "Speed: 4.0ms preprocess, 228.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 186.4ms\n",
      "Speed: 4.3ms preprocess, 186.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 139.9ms\n",
      "Speed: 4.8ms preprocess, 139.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 147.4ms\n",
      "Speed: 4.0ms preprocess, 147.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 140.8ms\n",
      "Speed: 3.0ms preprocess, 140.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 123.6ms\n",
      "Speed: 3.1ms preprocess, 123.6ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 127.5ms\n",
      "Speed: 4.0ms preprocess, 127.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 ties, 125.1ms\n",
      "Speed: 3.2ms preprocess, 125.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 115.9ms\n",
      "Speed: 4.5ms preprocess, 115.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 ties, 117.4ms\n",
      "Speed: 3.0ms preprocess, 117.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 cup, 116.8ms\n",
      "Speed: 3.5ms preprocess, 116.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 bowl, 112.6ms\n",
      "Speed: 3.0ms preprocess, 112.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 1 bowl, 120.1ms\n",
      "Speed: 4.0ms preprocess, 120.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 119.2ms\n",
      "Speed: 4.0ms preprocess, 119.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 150.1ms\n",
      "Speed: 4.0ms preprocess, 150.1ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 pizza, 127.9ms\n",
      "Speed: 4.0ms preprocess, 127.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 remote, 147.4ms\n",
      "Speed: 3.0ms preprocess, 147.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 126.0ms\n",
      "Speed: 3.4ms preprocess, 126.0ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 127.5ms\n",
      "Speed: 3.0ms preprocess, 127.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 124.8ms\n",
      "Speed: 3.7ms preprocess, 124.8ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 141.3ms\n",
      "Speed: 4.0ms preprocess, 141.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 118.0ms\n",
      "Speed: 3.0ms preprocess, 118.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 117.4ms\n",
      "Speed: 3.0ms preprocess, 117.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 117.2ms\n",
      "Speed: 4.0ms preprocess, 117.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 120.1ms\n",
      "Speed: 4.5ms preprocess, 120.1ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 124.3ms\n",
      "Speed: 3.2ms preprocess, 124.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 114.1ms\n",
      "Speed: 3.2ms preprocess, 114.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 1 cell phone, 148.2ms\n",
      "Speed: 4.3ms preprocess, 148.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 ties, 124.4ms\n",
      "Speed: 4.5ms preprocess, 124.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 118.5ms\n",
      "Speed: 3.0ms preprocess, 118.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 117.7ms\n",
      "Speed: 4.0ms preprocess, 117.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 donut, 122.3ms\n",
      "Speed: 4.0ms preprocess, 122.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 147.9ms\n",
      "Speed: 3.4ms preprocess, 147.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 131.2ms\n",
      "Speed: 3.0ms preprocess, 131.2ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 128.9ms\n",
      "Speed: 4.0ms preprocess, 128.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 121.6ms\n",
      "Speed: 5.0ms preprocess, 121.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 127.8ms\n",
      "Speed: 4.5ms preprocess, 127.8ms inference, 4.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 119.5ms\n",
      "Speed: 3.0ms preprocess, 119.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 121.9ms\n",
      "Speed: 3.0ms preprocess, 121.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 145.1ms\n",
      "Speed: 3.0ms preprocess, 145.1ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 135.4ms\n",
      "Speed: 4.0ms preprocess, 135.4ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 remote, 1 cell phone, 126.1ms\n",
      "Speed: 4.0ms preprocess, 126.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 cell phone, 119.7ms\n",
      "Speed: 3.7ms preprocess, 119.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 remote, 1 cell phone, 120.5ms\n",
      "Speed: 4.0ms preprocess, 120.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 remote, 119.8ms\n",
      "Speed: 3.0ms preprocess, 119.8ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 remote, 118.5ms\n",
      "Speed: 4.0ms preprocess, 118.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 remote, 1 cell phone, 120.2ms\n",
      "Speed: 4.0ms preprocess, 120.2ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 remote, 1 cell phone, 121.7ms\n",
      "Speed: 3.0ms preprocess, 121.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 remote, 1 cell phone, 136.5ms\n",
      "Speed: 3.0ms preprocess, 136.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 remote, 1 cell phone, 144.2ms\n",
      "Speed: 4.0ms preprocess, 144.2ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 remote, 146.5ms\n",
      "Speed: 4.0ms preprocess, 146.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 2 ties, 141.6ms\n",
      "Speed: 3.0ms preprocess, 141.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 ties, 128.2ms\n",
      "Speed: 4.0ms preprocess, 128.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 158.7ms\n",
      "Speed: 4.0ms preprocess, 158.7ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 170.5ms\n",
      "Speed: 4.0ms preprocess, 170.5ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 remote, 145.5ms\n",
      "Speed: 3.4ms preprocess, 145.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 125.3ms\n",
      "Speed: 4.0ms preprocess, 125.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 remote, 132.1ms\n",
      "Speed: 4.0ms preprocess, 132.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 cell phone, 119.5ms\n",
      "Speed: 4.0ms preprocess, 119.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 remote, 2 cell phones, 130.3ms\n",
      "Speed: 3.0ms preprocess, 130.3ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 2 cell phones, 148.9ms\n",
      "Speed: 3.4ms preprocess, 148.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 remote, 1 cell phone, 148.2ms\n",
      "Speed: 4.0ms preprocess, 148.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 remote, 1 cell phone, 148.6ms\n",
      "Speed: 3.0ms preprocess, 148.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 remote, 1 cell phone, 150.5ms\n",
      "Speed: 4.7ms preprocess, 150.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 remote, 1 cell phone, 135.9ms\n",
      "Speed: 4.3ms preprocess, 135.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 ties, 1 remote, 1 cell phone, 152.3ms\n",
      "Speed: 4.0ms preprocess, 152.3ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 remote, 1 cell phone, 145.3ms\n",
      "Speed: 4.0ms preprocess, 145.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 remote, 1 cell phone, 120.5ms\n",
      "Speed: 3.3ms preprocess, 120.5ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 remote, 1 cell phone, 117.0ms\n",
      "Speed: 3.5ms preprocess, 117.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 1 remote, 115.1ms\n",
      "Speed: 3.0ms preprocess, 115.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 ties, 1 remote, 1 cell phone, 120.0ms\n",
      "Speed: 3.0ms preprocess, 120.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 1 remote, 2 cell phones, 117.8ms\n",
      "Speed: 3.4ms preprocess, 117.8ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 remote, 2 cell phones, 121.0ms\n",
      "Speed: 4.0ms preprocess, 121.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 ties, 1 remote, 2 cell phones, 117.4ms\n",
      "Speed: 4.0ms preprocess, 117.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 remote, 1 cell phone, 116.0ms\n",
      "Speed: 3.9ms preprocess, 116.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 117.8ms\n",
      "Speed: 4.2ms preprocess, 117.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 remote, 1 cell phone, 126.7ms\n",
      "Speed: 3.5ms preprocess, 126.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 138.1ms\n",
      "Speed: 4.0ms preprocess, 138.1ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 ties, 1 remote, 1 cell phone, 123.9ms\n",
      "Speed: 4.0ms preprocess, 123.9ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 remote, 1 cell phone, 118.3ms\n",
      "Speed: 4.0ms preprocess, 118.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 remote, 1 cell phone, 137.3ms\n",
      "Speed: 4.4ms preprocess, 137.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 140.9ms\n",
      "Speed: 3.3ms preprocess, 140.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 143.8ms\n",
      "Speed: 3.0ms preprocess, 143.8ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 cell phone, 144.0ms\n",
      "Speed: 4.0ms preprocess, 144.0ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 1 cell phone, 128.2ms\n",
      "Speed: 4.0ms preprocess, 128.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 ties, 1 cell phone, 1 book, 109.1ms\n",
      "Speed: 3.5ms preprocess, 109.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 1 cell phone, 113.0ms\n",
      "Speed: 2.5ms preprocess, 113.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 remote, 1 cell phone, 129.4ms\n",
      "Speed: 3.0ms preprocess, 129.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 145.7ms\n",
      "Speed: 4.8ms preprocess, 145.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 121.4ms\n",
      "Speed: 6.3ms preprocess, 121.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 114.7ms\n",
      "Speed: 3.7ms preprocess, 114.7ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 cell phone, 118.3ms\n",
      "Speed: 4.0ms preprocess, 118.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 119.3ms\n",
      "Speed: 3.3ms preprocess, 119.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 142.4ms\n",
      "Speed: 3.5ms preprocess, 142.4ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 donut, 1 chair, 159.4ms\n",
      "Speed: 3.0ms preprocess, 159.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 1 donut, 123.5ms\n",
      "Speed: 4.0ms preprocess, 123.5ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 donut, 1 chair, 141.0ms\n",
      "Speed: 4.6ms preprocess, 141.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 donut, 139.4ms\n",
      "Speed: 4.2ms preprocess, 139.4ms inference, 11.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 152.1ms\n",
      "Speed: 4.0ms preprocess, 152.1ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 151.7ms\n",
      "Speed: 4.0ms preprocess, 151.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 remote, 122.5ms\n",
      "Speed: 4.2ms preprocess, 122.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 remote, 117.5ms\n",
      "Speed: 3.0ms preprocess, 117.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 remote, 115.3ms\n",
      "Speed: 3.4ms preprocess, 115.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 117.8ms\n",
      "Speed: 3.7ms preprocess, 117.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 125.7ms\n",
      "Speed: 4.0ms preprocess, 125.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 120.9ms\n",
      "Speed: 3.3ms preprocess, 120.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 116.3ms\n",
      "Speed: 4.5ms preprocess, 116.3ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 117.3ms\n",
      "Speed: 4.0ms preprocess, 117.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 124.6ms\n",
      "Speed: 3.5ms preprocess, 124.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 154.0ms\n",
      "Speed: 4.0ms preprocess, 154.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 125.2ms\n",
      "Speed: 4.7ms preprocess, 125.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 117.3ms\n",
      "Speed: 4.0ms preprocess, 117.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 donut, 122.4ms\n",
      "Speed: 3.4ms preprocess, 122.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 donut, 139.0ms\n",
      "Speed: 4.0ms preprocess, 139.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 donut, 142.3ms\n",
      "Speed: 3.5ms preprocess, 142.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 135.1ms\n",
      "Speed: 4.0ms preprocess, 135.1ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 remote, 119.4ms\n",
      "Speed: 3.5ms preprocess, 119.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 donut, 117.4ms\n",
      "Speed: 4.8ms preprocess, 117.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 125.1ms\n",
      "Speed: 3.3ms preprocess, 125.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 146.7ms\n",
      "Speed: 4.0ms preprocess, 146.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 129.4ms\n",
      "Speed: 4.0ms preprocess, 129.4ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 121.9ms\n",
      "Speed: 4.0ms preprocess, 121.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 142.1ms\n",
      "Speed: 3.2ms preprocess, 142.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 153.1ms\n",
      "Speed: 3.3ms preprocess, 153.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 ties, 1 wine glass, 126.1ms\n",
      "Speed: 4.0ms preprocess, 126.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 ties, 122.3ms\n",
      "Speed: 3.5ms preprocess, 122.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 121.7ms\n",
      "Speed: 3.3ms preprocess, 121.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 121.0ms\n",
      "Speed: 4.0ms preprocess, 121.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 157.2ms\n",
      "Speed: 3.8ms preprocess, 157.2ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 147.6ms\n",
      "Speed: 4.0ms preprocess, 147.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 155.0ms\n",
      "Speed: 4.0ms preprocess, 155.0ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 130.0ms\n",
      "Speed: 4.0ms preprocess, 130.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 119.5ms\n",
      "Speed: 3.5ms preprocess, 119.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 cell phone, 117.6ms\n",
      "Speed: 4.0ms preprocess, 117.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 119.7ms\n",
      "Speed: 4.0ms preprocess, 119.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 118.5ms\n",
      "Speed: 4.0ms preprocess, 118.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 120.8ms\n",
      "Speed: 3.3ms preprocess, 120.8ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 164.2ms\n",
      "Speed: 3.7ms preprocess, 164.2ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 130.3ms\n",
      "Speed: 4.1ms preprocess, 130.3ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 3 ties, 121.6ms\n",
      "Speed: 3.2ms preprocess, 121.6ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 donut, 116.4ms\n",
      "Speed: 4.2ms preprocess, 116.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 115.5ms\n",
      "Speed: 4.4ms preprocess, 115.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 3 ties, 116.7ms\n",
      "Speed: 3.5ms preprocess, 116.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 118.6ms\n",
      "Speed: 3.0ms preprocess, 118.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 119.6ms\n",
      "Speed: 3.4ms preprocess, 119.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 158.4ms\n",
      "Speed: 3.9ms preprocess, 158.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 170.4ms\n",
      "Speed: 4.0ms preprocess, 170.4ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 139.2ms\n",
      "Speed: 3.5ms preprocess, 139.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 132.3ms\n",
      "Speed: 3.0ms preprocess, 132.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 121.1ms\n",
      "Speed: 4.0ms preprocess, 121.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 2 cell phones, 137.0ms\n",
      "Speed: 3.0ms preprocess, 137.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 117.6ms\n",
      "Speed: 3.0ms preprocess, 117.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 dining table, 120.5ms\n",
      "Speed: 3.5ms preprocess, 120.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 122.1ms\n",
      "Speed: 3.0ms preprocess, 122.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 118.6ms\n",
      "Speed: 3.9ms preprocess, 118.6ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 2 donuts, 156.3ms\n",
      "Speed: 4.0ms preprocess, 156.3ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 127.6ms\n",
      "Speed: 4.0ms preprocess, 127.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 1 donut, 118.4ms\n",
      "Speed: 4.3ms preprocess, 118.4ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 126.0ms\n",
      "Speed: 4.4ms preprocess, 126.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 handbag, 1 tie, 115.5ms\n",
      "Speed: 4.0ms preprocess, 115.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 118.6ms\n",
      "Speed: 3.4ms preprocess, 118.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 121.7ms\n",
      "Speed: 3.0ms preprocess, 121.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 137.0ms\n",
      "Speed: 4.0ms preprocess, 137.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 180.0ms\n",
      "Speed: 4.3ms preprocess, 180.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 153.9ms\n",
      "Speed: 4.4ms preprocess, 153.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 137.2ms\n",
      "Speed: 3.5ms preprocess, 137.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 120.7ms\n",
      "Speed: 3.2ms preprocess, 120.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 dining table, 130.7ms\n",
      "Speed: 4.0ms preprocess, 130.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 1 dining table, 154.3ms\n",
      "Speed: 3.0ms preprocess, 154.3ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 127.2ms\n",
      "Speed: 4.0ms preprocess, 127.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 127.2ms\n",
      "Speed: 3.7ms preprocess, 127.2ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 148.9ms\n",
      "Speed: 4.3ms preprocess, 148.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 130.5ms\n",
      "Speed: 3.0ms preprocess, 130.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 123.5ms\n",
      "Speed: 3.7ms preprocess, 123.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 122.0ms\n",
      "Speed: 3.3ms preprocess, 122.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 118.0ms\n",
      "Speed: 3.0ms preprocess, 118.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 117.4ms\n",
      "Speed: 4.0ms preprocess, 117.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 120.4ms\n",
      "Speed: 3.6ms preprocess, 120.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 125.7ms\n",
      "Speed: 4.0ms preprocess, 125.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 158.2ms\n",
      "Speed: 3.3ms preprocess, 158.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 167.8ms\n",
      "Speed: 4.5ms preprocess, 167.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 139.5ms\n",
      "Speed: 4.0ms preprocess, 139.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 132.8ms\n",
      "Speed: 4.0ms preprocess, 132.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 116.5ms\n",
      "Speed: 3.6ms preprocess, 116.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 123.0ms\n",
      "Speed: 3.6ms preprocess, 123.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 120.9ms\n",
      "Speed: 3.8ms preprocess, 120.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 119.0ms\n",
      "Speed: 3.4ms preprocess, 119.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 158.3ms\n",
      "Speed: 4.0ms preprocess, 158.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 133.1ms\n",
      "Speed: 4.4ms preprocess, 133.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 116.6ms\n",
      "Speed: 3.0ms preprocess, 116.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 117.5ms\n",
      "Speed: 4.0ms preprocess, 117.5ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 120.6ms\n",
      "Speed: 3.3ms preprocess, 120.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 dining table, 121.0ms\n",
      "Speed: 3.0ms preprocess, 121.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 128.3ms\n",
      "Speed: 3.0ms preprocess, 128.3ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 152.9ms\n",
      "Speed: 4.0ms preprocess, 152.9ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 162.8ms\n",
      "Speed: 3.3ms preprocess, 162.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 160.5ms\n",
      "Speed: 3.2ms preprocess, 160.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 151.6ms\n",
      "Speed: 4.0ms preprocess, 151.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 137.7ms\n",
      "Speed: 3.0ms preprocess, 137.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 124.6ms\n",
      "Speed: 4.0ms preprocess, 124.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 122.8ms\n",
      "Speed: 4.6ms preprocess, 122.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 122.8ms\n",
      "Speed: 3.2ms preprocess, 122.8ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 150.2ms\n",
      "Speed: 3.3ms preprocess, 150.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 129.2ms\n",
      "Speed: 4.0ms preprocess, 129.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 bowl, 120.7ms\n",
      "Speed: 4.3ms preprocess, 120.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 bowl, 116.9ms\n",
      "Speed: 4.5ms preprocess, 116.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 bowl, 119.0ms\n",
      "Speed: 4.0ms preprocess, 119.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 118.6ms\n",
      "Speed: 4.3ms preprocess, 118.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 121.6ms\n",
      "Speed: 4.0ms preprocess, 121.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 121.2ms\n",
      "Speed: 4.0ms preprocess, 121.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 bowl, 166.6ms\n",
      "Speed: 3.0ms preprocess, 166.6ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 bowl, 123.1ms\n",
      "Speed: 3.3ms preprocess, 123.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 bowl, 150.4ms\n",
      "Speed: 3.0ms preprocess, 150.4ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 bowl, 151.0ms\n",
      "Speed: 3.0ms preprocess, 151.0ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 bowl, 136.2ms\n",
      "Speed: 4.3ms preprocess, 136.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 bowl, 117.0ms\n",
      "Speed: 4.0ms preprocess, 117.0ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 bowl, 119.6ms\n",
      "Speed: 3.5ms preprocess, 119.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 bowl, 151.7ms\n",
      "Speed: 3.0ms preprocess, 151.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 handbag, 1 bowl, 129.8ms\n",
      "Speed: 3.5ms preprocess, 129.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 bowl, 122.1ms\n",
      "Speed: 3.0ms preprocess, 122.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 bowl, 117.0ms\n",
      "Speed: 3.0ms preprocess, 117.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 bowl, 121.0ms\n",
      "Speed: 4.0ms preprocess, 121.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 chair, 119.3ms\n",
      "Speed: 4.0ms preprocess, 119.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 117.6ms\n",
      "Speed: 4.2ms preprocess, 117.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 bowl, 121.0ms\n",
      "Speed: 4.0ms preprocess, 121.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 157.4ms\n",
      "Speed: 3.0ms preprocess, 157.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 127.3ms\n",
      "Speed: 3.0ms preprocess, 127.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 117.6ms\n",
      "Speed: 4.2ms preprocess, 117.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 bowl, 129.9ms\n",
      "Speed: 4.3ms preprocess, 129.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 bowl, 158.4ms\n",
      "Speed: 3.0ms preprocess, 158.4ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 bowl, 136.1ms\n",
      "Speed: 3.3ms preprocess, 136.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 bowl, 1 laptop, 131.0ms\n",
      "Speed: 3.5ms preprocess, 131.0ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 bowl, 170.6ms\n",
      "Speed: 4.3ms preprocess, 170.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 119.6ms\n",
      "Speed: 5.0ms preprocess, 119.6ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 bowl, 124.1ms\n",
      "Speed: 3.0ms preprocess, 124.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 123.5ms\n",
      "Speed: 4.1ms preprocess, 123.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 122.7ms\n",
      "Speed: 4.0ms preprocess, 122.7ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 117.8ms\n",
      "Speed: 4.4ms preprocess, 117.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 123.0ms\n",
      "Speed: 4.0ms preprocess, 123.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 151.5ms\n",
      "Speed: 3.8ms preprocess, 151.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 128.7ms\n",
      "Speed: 3.5ms preprocess, 128.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 136.4ms\n",
      "Speed: 4.0ms preprocess, 136.4ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 154.3ms\n",
      "Speed: 4.2ms preprocess, 154.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 119.4ms\n",
      "Speed: 3.6ms preprocess, 119.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 129.9ms\n",
      "Speed: 4.5ms preprocess, 129.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 165.7ms\n",
      "Speed: 3.0ms preprocess, 165.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 183.3ms\n",
      "Speed: 3.5ms preprocess, 183.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 139.6ms\n",
      "Speed: 4.0ms preprocess, 139.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 120.3ms\n",
      "Speed: 4.0ms preprocess, 120.3ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 117.8ms\n",
      "Speed: 4.6ms preprocess, 117.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 118.7ms\n",
      "Speed: 3.0ms preprocess, 118.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 119.7ms\n",
      "Speed: 4.3ms preprocess, 119.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 122.7ms\n",
      "Speed: 4.3ms preprocess, 122.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 120.6ms\n",
      "Speed: 4.4ms preprocess, 120.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 155.9ms\n",
      "Speed: 4.7ms preprocess, 155.9ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 137.5ms\n",
      "Speed: 4.0ms preprocess, 137.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 124.2ms\n",
      "Speed: 4.3ms preprocess, 124.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 121.3ms\n",
      "Speed: 4.0ms preprocess, 121.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 117.7ms\n",
      "Speed: 4.0ms preprocess, 117.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 117.8ms\n",
      "Speed: 3.5ms preprocess, 117.8ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 120.1ms\n",
      "Speed: 4.0ms preprocess, 120.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 134.3ms\n",
      "Speed: 3.5ms preprocess, 134.3ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 173.5ms\n",
      "Speed: 4.5ms preprocess, 173.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 167.7ms\n",
      "Speed: 4.4ms preprocess, 167.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 160.1ms\n",
      "Speed: 4.6ms preprocess, 160.1ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 157.7ms\n",
      "Speed: 4.0ms preprocess, 157.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 168.7ms\n",
      "Speed: 3.4ms preprocess, 168.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 160.7ms\n",
      "Speed: 3.0ms preprocess, 160.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 179.0ms\n",
      "Speed: 4.4ms preprocess, 179.0ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 ties, 161.5ms\n",
      "Speed: 4.0ms preprocess, 161.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 153.2ms\n",
      "Speed: 4.0ms preprocess, 153.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 154.0ms\n",
      "Speed: 3.0ms preprocess, 154.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 mouse, 148.9ms\n",
      "Speed: 2.8ms preprocess, 148.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 mouse, 146.8ms\n",
      "Speed: 4.0ms preprocess, 146.8ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 1 mouse, 149.1ms\n",
      "Speed: 3.0ms preprocess, 149.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 mouse, 168.0ms\n",
      "Speed: 3.0ms preprocess, 168.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 161.4ms\n",
      "Speed: 4.3ms preprocess, 161.4ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 166.7ms\n",
      "Speed: 4.3ms preprocess, 166.7ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 3 ties, 163.2ms\n",
      "Speed: 4.3ms preprocess, 163.2ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 1 mouse, 133.4ms\n",
      "Speed: 3.0ms preprocess, 133.4ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 118.4ms\n",
      "Speed: 3.0ms preprocess, 118.4ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 mouse, 122.1ms\n",
      "Speed: 3.0ms preprocess, 122.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 139.8ms\n",
      "Speed: 3.0ms preprocess, 139.8ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 135.3ms\n",
      "Speed: 3.5ms preprocess, 135.3ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 3 ties, 121.3ms\n",
      "Speed: 4.0ms preprocess, 121.3ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 mouse, 1 remote, 119.9ms\n",
      "Speed: 3.0ms preprocess, 119.9ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 4 ties, 1 mouse, 117.7ms\n",
      "Speed: 4.2ms preprocess, 117.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 125.4ms\n",
      "Speed: 4.0ms preprocess, 125.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 2 ties, 150.1ms\n",
      "Speed: 4.3ms preprocess, 150.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 137.7ms\n",
      "Speed: 3.0ms preprocess, 137.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 155.5ms\n",
      "Speed: 3.3ms preprocess, 155.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 136.3ms\n",
      "Speed: 4.0ms preprocess, 136.3ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 3 ties, 119.5ms\n",
      "Speed: 3.0ms preprocess, 119.5ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 ties, 136.4ms\n",
      "Speed: 4.2ms preprocess, 136.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 157.0ms\n",
      "Speed: 4.2ms preprocess, 157.0ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 133.3ms\n",
      "Speed: 3.0ms preprocess, 133.3ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 119.8ms\n",
      "Speed: 3.4ms preprocess, 119.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 140.8ms\n",
      "Speed: 4.0ms preprocess, 140.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 137.6ms\n",
      "Speed: 4.0ms preprocess, 137.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 118.1ms\n",
      "Speed: 4.0ms preprocess, 118.1ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 129.3ms\n",
      "Speed: 4.0ms preprocess, 129.3ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 121.9ms\n",
      "Speed: 4.1ms preprocess, 121.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 120.8ms\n",
      "Speed: 3.4ms preprocess, 120.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 119.5ms\n",
      "Speed: 4.0ms preprocess, 119.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 119.6ms\n",
      "Speed: 3.3ms preprocess, 119.6ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 147.7ms\n",
      "Speed: 3.0ms preprocess, 147.7ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 142.8ms\n",
      "Speed: 3.0ms preprocess, 142.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 122.6ms\n",
      "Speed: 3.3ms preprocess, 122.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 122.2ms\n",
      "Speed: 4.3ms preprocess, 122.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 124.9ms\n",
      "Speed: 3.0ms preprocess, 124.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 ties, 153.3ms\n",
      "Speed: 4.0ms preprocess, 153.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 140.5ms\n",
      "Speed: 3.0ms preprocess, 140.5ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 158.3ms\n",
      "Speed: 4.0ms preprocess, 158.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 160.7ms\n",
      "Speed: 3.0ms preprocess, 160.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 125.9ms\n",
      "Speed: 3.0ms preprocess, 125.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 120.1ms\n",
      "Speed: 4.0ms preprocess, 120.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 119.6ms\n",
      "Speed: 3.0ms preprocess, 119.6ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 116.3ms\n",
      "Speed: 4.0ms preprocess, 116.3ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 122.1ms\n",
      "Speed: 3.0ms preprocess, 122.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 ties, 119.6ms\n",
      "Speed: 3.3ms preprocess, 119.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 155.8ms\n",
      "Speed: 4.0ms preprocess, 155.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 1 cell phone, 127.1ms\n",
      "Speed: 3.6ms preprocess, 127.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 cell phone, 127.2ms\n",
      "Speed: 4.0ms preprocess, 127.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 120.6ms\n",
      "Speed: 3.3ms preprocess, 120.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 3 ties, 119.4ms\n",
      "Speed: 4.4ms preprocess, 119.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 116.8ms\n",
      "Speed: 4.0ms preprocess, 116.8ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 156.2ms\n",
      "Speed: 3.3ms preprocess, 156.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 167.8ms\n",
      "Speed: 3.5ms preprocess, 167.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 2 donuts, 144.3ms\n",
      "Speed: 4.0ms preprocess, 144.3ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 donut, 124.2ms\n",
      "Speed: 4.0ms preprocess, 124.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 cell phone, 120.1ms\n",
      "Speed: 3.0ms preprocess, 120.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 122.0ms\n",
      "Speed: 4.5ms preprocess, 122.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 119.9ms\n",
      "Speed: 4.0ms preprocess, 119.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 120.9ms\n",
      "Speed: 4.3ms preprocess, 120.9ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 116.7ms\n",
      "Speed: 4.0ms preprocess, 116.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 ties, 1 donut, 1 cell phone, 147.3ms\n",
      "Speed: 3.0ms preprocess, 147.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 129.3ms\n",
      "Speed: 3.3ms preprocess, 129.3ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 132.0ms\n",
      "Speed: 4.3ms preprocess, 132.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 cell phone, 116.2ms\n",
      "Speed: 4.4ms preprocess, 116.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 120.7ms\n",
      "Speed: 2.6ms preprocess, 120.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 ties, 1 cell phone, 118.6ms\n",
      "Speed: 4.0ms preprocess, 118.6ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 cell phone, 117.4ms\n",
      "Speed: 4.0ms preprocess, 117.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 1 donut, 1 cell phone, 118.9ms\n",
      "Speed: 4.3ms preprocess, 118.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 donut, 148.5ms\n",
      "Speed: 4.0ms preprocess, 148.5ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 donut, 1 cell phone, 174.0ms\n",
      "Speed: 3.1ms preprocess, 174.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 2 ties, 1 donut, 1 cell phone, 143.6ms\n",
      "Speed: 4.0ms preprocess, 143.6ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 remote, 132.7ms\n",
      "Speed: 4.0ms preprocess, 132.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 remote, 119.9ms\n",
      "Speed: 3.0ms preprocess, 119.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 117.0ms\n",
      "Speed: 3.0ms preprocess, 117.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 donut, 116.6ms\n",
      "Speed: 4.5ms preprocess, 116.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 ties, 121.6ms\n",
      "Speed: 4.0ms preprocess, 121.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 3 ties, 140.8ms\n",
      "Speed: 4.0ms preprocess, 140.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 remote, 1 cell phone, 130.2ms\n",
      "Speed: 4.7ms preprocess, 130.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 donut, 127.6ms\n",
      "Speed: 4.0ms preprocess, 127.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 donut, 116.5ms\n",
      "Speed: 4.0ms preprocess, 116.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 donut, 120.1ms\n",
      "Speed: 4.5ms preprocess, 120.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 donut, 118.7ms\n",
      "Speed: 3.0ms preprocess, 118.7ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 donut, 120.8ms\n",
      "Speed: 3.0ms preprocess, 120.8ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 donut, 115.9ms\n",
      "Speed: 3.3ms preprocess, 115.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 1 cell phone, 125.0ms\n",
      "Speed: 3.2ms preprocess, 125.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 ties, 1 cell phone, 166.9ms\n",
      "Speed: 3.5ms preprocess, 166.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 3 ties, 1 cup, 1 cell phone, 166.7ms\n",
      "Speed: 4.1ms preprocess, 166.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 ties, 1 cell phone, 130.3ms\n",
      "Speed: 3.0ms preprocess, 130.3ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 130.9ms\n",
      "Speed: 3.5ms preprocess, 130.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 115.6ms\n",
      "Speed: 3.0ms preprocess, 115.6ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 cell phone, 118.6ms\n",
      "Speed: 3.5ms preprocess, 118.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 ties, 1 wine glass, 119.6ms\n",
      "Speed: 4.0ms preprocess, 119.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 cell phone, 123.3ms\n",
      "Speed: 4.0ms preprocess, 123.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 ties, 1 cup, 1 cell phone, 157.3ms\n",
      "Speed: 4.0ms preprocess, 157.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 134.6ms\n",
      "Speed: 3.0ms preprocess, 134.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 ties, 1 cell phone, 120.7ms\n",
      "Speed: 3.3ms preprocess, 120.7ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 cell phone, 119.7ms\n",
      "Speed: 3.4ms preprocess, 119.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 120.6ms\n",
      "Speed: 4.0ms preprocess, 120.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 ties, 1 cell phone, 116.9ms\n",
      "Speed: 3.0ms preprocess, 116.9ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 1 cell phone, 121.6ms\n",
      "Speed: 4.3ms preprocess, 121.6ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 121.6ms\n",
      "Speed: 3.4ms preprocess, 121.6ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 1 cell phone, 153.4ms\n",
      "Speed: 3.0ms preprocess, 153.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 153.7ms\n",
      "Speed: 4.0ms preprocess, 153.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 cup, 158.3ms\n",
      "Speed: 4.0ms preprocess, 158.3ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 ties, 135.8ms\n",
      "Speed: 3.2ms preprocess, 135.8ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 123.4ms\n",
      "Speed: 3.5ms preprocess, 123.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 2 ties, 1 bowl, 119.7ms\n",
      "Speed: 4.3ms preprocess, 119.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 wine glass, 120.4ms\n",
      "Speed: 4.0ms preprocess, 120.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 bowl, 129.5ms\n",
      "Speed: 4.0ms preprocess, 129.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 tie, 1 bowl, 155.7ms\n",
      "Speed: 4.0ms preprocess, 155.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 cup, 1 bowl, 128.1ms\n",
      "Speed: 4.6ms preprocess, 128.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 donut, 118.1ms\n",
      "Speed: 3.0ms preprocess, 118.1ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 bowl, 128.1ms\n",
      "Speed: 3.3ms preprocess, 128.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 1 bowl, 149.1ms\n",
      "Speed: 4.0ms preprocess, 149.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 1 bowl, 117.8ms\n",
      "Speed: 3.3ms preprocess, 117.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 bowl, 116.0ms\n",
      "Speed: 4.0ms preprocess, 116.0ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 bowl, 161.4ms\n",
      "Speed: 4.4ms preprocess, 161.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 bowl, 126.8ms\n",
      "Speed: 5.3ms preprocess, 126.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 bowl, 146.5ms\n",
      "Speed: 4.4ms preprocess, 146.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 cell phone, 151.7ms\n",
      "Speed: 3.5ms preprocess, 151.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 136.0ms\n",
      "Speed: 4.0ms preprocess, 136.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 1 donut, 117.4ms\n",
      "Speed: 4.0ms preprocess, 117.4ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 1 donut, 116.6ms\n",
      "Speed: 4.4ms preprocess, 116.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 150.8ms\n",
      "Speed: 4.0ms preprocess, 150.8ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 133.8ms\n",
      "Speed: 3.0ms preprocess, 133.8ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 3 ties, 117.6ms\n",
      "Speed: 5.2ms preprocess, 117.6ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 tie, 118.7ms\n",
      "Speed: 3.7ms preprocess, 118.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 119.2ms\n",
      "Speed: 3.0ms preprocess, 119.2ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 118.0ms\n",
      "Speed: 4.0ms preprocess, 118.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 1 vase, 118.7ms\n",
      "Speed: 4.0ms preprocess, 118.7ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 1 vase, 120.6ms\n",
      "Speed: 2.8ms preprocess, 120.6ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 1 vase, 162.3ms\n",
      "Speed: 3.3ms preprocess, 162.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 1 vase, 122.7ms\n",
      "Speed: 4.6ms preprocess, 122.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 ties, 118.6ms\n",
      "Speed: 4.2ms preprocess, 118.6ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "class ObjectDetectionTracker:\n",
    "    def __init__(self, model_path='yolov8n.pt', confidence_threshold=0.5, iou_threshold=0.3):\n",
    "        \"\"\"\n",
    "        Initialize the object detection and tracking system\n",
    "        \"\"\"\n",
    "        self.model = YOLO(model_path)\n",
    "        self.conf_threshold = confidence_threshold\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.tracking_objects = {}\n",
    "        self.track_id = 0\n",
    "        self.fps_history = []\n",
    "        \n",
    "    def calculate_iou(self, box1, box2):\n",
    "        \"\"\"Calculate Intersection over Union between two boxes\"\"\"\n",
    "        x1 = max(box1[0], box2[0])\n",
    "        y1 = max(box1[1], box2[1])\n",
    "        x2 = min(box1[2], box2[2])\n",
    "        y2 = min(box1[3], box2[3])\n",
    "        \n",
    "        intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        union = area1 + area2 - intersection\n",
    "        \n",
    "        return intersection / union if union > 0 else 0\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Process a single frame for object detection and tracking\n",
    "        \"\"\"\n",
    "        # Store original frame dimensions\n",
    "        height, width = frame.shape[:2]\n",
    "        \n",
    "        # Detect objects\n",
    "        results = self.model(frame, stream=True)\n",
    "        \n",
    "        # Process detection results\n",
    "        current_objects = {}\n",
    "        \n",
    "        for result in results:\n",
    "            boxes = result.boxes.cpu().numpy()\n",
    "            \n",
    "            for box in boxes:\n",
    "                # Get detection information\n",
    "                x1, y1, x2, y2 = box.xyxy[0].astype(int)\n",
    "                conf = box.conf[0]\n",
    "                class_id = int(box.cls[0])\n",
    "                class_name = self.model.names[class_id]\n",
    "                \n",
    "                if conf < self.conf_threshold:\n",
    "                    continue\n",
    "                    \n",
    "                # Check if object overlaps with any existing tracked object\n",
    "                matched = False\n",
    "                for track_id, tracked_obj in self.tracking_objects.items():\n",
    "                    tracked_box = tracked_obj['box']\n",
    "                    iou = self.calculate_iou([x1, y1, x2, y2], tracked_box)\n",
    "                    \n",
    "                    if iou > self.iou_threshold:\n",
    "                        current_objects[track_id] = {\n",
    "                            'box': [x1, y1, x2, y2],\n",
    "                            'class': class_name,\n",
    "                            'confidence': conf,\n",
    "                            'tracked_frames': tracked_obj['tracked_frames'] + 1\n",
    "                        }\n",
    "                        matched = True\n",
    "                        break\n",
    "                \n",
    "                # If no match found, create new tracked object\n",
    "                if not matched:\n",
    "                    self.track_id += 1\n",
    "                    current_objects[self.track_id] = {\n",
    "                        'box': [x1, y1, x2, y2],\n",
    "                        'class': class_name,\n",
    "                        'confidence': conf,\n",
    "                        'tracked_frames': 1\n",
    "                    }\n",
    "        \n",
    "        # Update tracking objects\n",
    "        self.tracking_objects = current_objects\n",
    "        \n",
    "        return self.tracking_objects\n",
    "\n",
    "    def draw_results(self, frame, objects):\n",
    "        \"\"\"\n",
    "        Draw detection and tracking results on frame\n",
    "        \"\"\"\n",
    "        for track_id, obj in objects.items():\n",
    "            x1, y1, x2, y2 = obj['box']\n",
    "            class_name = obj['class']\n",
    "            conf = obj['confidence']\n",
    "            tracked_frames = obj['tracked_frames']\n",
    "            \n",
    "            # Draw box with different colors based on tracking duration\n",
    "            color = (0, int(min(255, tracked_frames * 10)), 0)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            \n",
    "            # Draw label with tracking ID\n",
    "            label = f\"#{track_id} {class_name} {conf:.2f}\"\n",
    "            cv2.putText(frame, label, (x1, y1-10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "            \n",
    "        return frame\n",
    "\n",
    "    def run_detection(self, source=0, display=True):\n",
    "        \"\"\"\n",
    "        Run real-time detection and tracking\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(source)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Couldn't open video source\")\n",
    "            return\n",
    "        \n",
    "        print(\"Detection and tracking started... Press 'q' to quit\")\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                # Process frame\n",
    "                tracked_objects = self.process_frame(frame)\n",
    "                \n",
    "                # Draw results\n",
    "                if display:\n",
    "                    frame = self.draw_results(frame, tracked_objects)\n",
    "                    \n",
    "                    # Calculate and display FPS\n",
    "                    fps = 1.0 / (time.time() - start_time)\n",
    "                    self.fps_history.append(fps)\n",
    "                    if len(self.fps_history) > 30:\n",
    "                        self.fps_history.pop(0)\n",
    "                    avg_fps = sum(self.fps_history) / len(self.fps_history)\n",
    "                    \n",
    "                    cv2.putText(frame, f'FPS: {avg_fps:.1f}', (10, 30),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                    \n",
    "                    cv2.imshow('Object Detection and Tracking', frame)\n",
    "                \n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "        finally:\n",
    "            cap.release()\n",
    "            if display:\n",
    "                cv2.destroyAllWindows()\n",
    "\n",
    "def main():\n",
    "    # Initialize detector with custom settings\n",
    "    detector = ObjectDetectionTracker(\n",
    "        model_path='yolov8n.pt',\n",
    "        confidence_threshold=0.5,\n",
    "        iou_threshold=0.3\n",
    "    )\n",
    "    \n",
    "    # Run detection\n",
    "    detector.run_detection()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55d327c7-8024-432c-a502-89fab462ce3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'yolov8n_quantized.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 194\u001b[0m\n\u001b[0;32m    191\u001b[0m     detector\u001b[38;5;241m.\u001b[39mrun_detection()\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 194\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[2], line 184\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Initialize detector with quantized model\u001b[39;00m\n\u001b[1;32m--> 184\u001b[0m     detector \u001b[38;5;241m=\u001b[39m ObjectDetectionTracker(\n\u001b[0;32m    185\u001b[0m         model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov8n_quantized.pt\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Using the quantized model\u001b[39;00m\n\u001b[0;32m    186\u001b[0m         confidence_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m    187\u001b[0m         iou_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;66;03m# Run detection\u001b[39;00m\n\u001b[0;32m    191\u001b[0m     detector\u001b[38;5;241m.\u001b[39mrun_detection()\n",
      "Cell \u001b[1;32mIn[2], line 20\u001b[0m, in \u001b[0;36mObjectDetectionTracker.__init__\u001b[1;34m(self, model_path, confidence_threshold, iou_threshold)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03mInitialize the object detection and tracking system with quantized model\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# First try loading directly\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m YOLO(model_path)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# If that fails, try loading as a state dict\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to load quantized model as state dict...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\models\\yolo\\model.py:23\u001b[0m, in \u001b[0;36mYOLO.__init__\u001b[1;34m(self, model, task, verbose)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m \u001b[38;5;241m=\u001b[39m new_instance\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Continue with default YOLO initialization\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(model\u001b[38;5;241m=\u001b[39mmodel, task\u001b[38;5;241m=\u001b[39mtask, verbose\u001b[38;5;241m=\u001b[39mverbose)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\model.py:145\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, model, task, verbose)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(model, task\u001b[38;5;241m=\u001b[39mtask, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load(model, task\u001b[38;5;241m=\u001b[39mtask)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\model.py:285\u001b[0m, in \u001b[0;36mModel._load\u001b[1;34m(self, weights, task)\u001b[0m\n\u001b[0;32m    282\u001b[0m weights \u001b[38;5;241m=\u001b[39m checks\u001b[38;5;241m.\u001b[39mcheck_model_file_from_stem(weights)  \u001b[38;5;66;03m# add suffix, i.e. yolov8n -> yolov8n.pt\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(weights)\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;241m=\u001b[39m attempt_load_one_weight(weights)\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_ckpt_args(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\tasks.py:910\u001b[0m, in \u001b[0;36mattempt_load_one_weight\u001b[1;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattempt_load_one_weight\u001b[39m(weight, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fuse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    909\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a single model weights.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 910\u001b[0m     ckpt, weight \u001b[38;5;241m=\u001b[39m torch_safe_load(weight)  \u001b[38;5;66;03m# load ckpt\u001b[39;00m\n\u001b[0;32m    911\u001b[0m     args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mDEFAULT_CFG_DICT, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_args\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))}  \u001b[38;5;66;03m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[0;32m    912\u001b[0m     model \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\nn\\tasks.py:837\u001b[0m, in \u001b[0;36mtorch_safe_load\u001b[1;34m(weight, safe_only)\u001b[0m\n\u001b[0;32m    835\u001b[0m                 ckpt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(f, pickle_module\u001b[38;5;241m=\u001b[39msafe_pickle)\n\u001b[0;32m    836\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 837\u001b[0m             ckpt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(file, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    839\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# e.name is missing module name\u001b[39;00m\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\utils\\patches.py:86\u001b[0m, in \u001b[0;36mtorch_load\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TORCH_1_13 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m     84\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _torch_load(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'yolov8n_quantized.pt'"
     ]
    }
   ],
   "source": [
    "#quantized model\n",
    "\n",
    "#i downloaded the quantized version  by quantizing\n",
    "# size reduced to half s\n",
    "\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import torch\n",
    "\n",
    "class ObjectDetectionTracker:\n",
    "    def __init__(self, model_path='yolov8n_quantized.pt', confidence_threshold=0.5, iou_threshold=0.3):\n",
    "        \"\"\"\n",
    "        Initialize the object detection and tracking system with quantized model\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # First try loading directly\n",
    "            self.model = YOLO(model_path)\n",
    "        except KeyError:\n",
    "            # If that fails, try loading as a state dict\n",
    "            print(\"Attempting to load quantized model as state dict...\")\n",
    "            base_model = YOLO('yolov8n.pt')  # Load base model first\n",
    "            state_dict = torch.load(model_path)\n",
    "            \n",
    "            # Handle different possible state dict formats\n",
    "            if isinstance(state_dict, dict):\n",
    "                if 'state_dict' in state_dict:\n",
    "                    state_dict = state_dict['state_dict']\n",
    "                elif 'model' in state_dict:\n",
    "                    state_dict = state_dict['model']\n",
    "            \n",
    "            base_model.model.load_state_dict(state_dict, strict=False)\n",
    "            self.model = base_model\n",
    "            \n",
    "        self.conf_threshold = confidence_threshold\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.tracking_objects = {}\n",
    "        self.track_id = 0\n",
    "        self.fps_history = []\n",
    "        \n",
    "    def calculate_iou(self, box1, box2):\n",
    "        \"\"\"Calculate Intersection over Union between two boxes\"\"\"\n",
    "        x1 = max(box1[0], box2[0])\n",
    "        y1 = max(box1[1], box2[1])\n",
    "        x2 = min(box1[2], box2[2])\n",
    "        y2 = min(box1[3], box2[3])\n",
    "        \n",
    "        intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        union = area1 + area2 - intersection\n",
    "        \n",
    "        return intersection / union if union > 0 else 0\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Process a single frame for object detection and tracking\n",
    "        \"\"\"\n",
    "        # Store original frame dimensions\n",
    "        height, width = frame.shape[:2]\n",
    "        \n",
    "        # Detect objects\n",
    "        results = self.model(frame, stream=True)\n",
    "        \n",
    "        # Process detection results\n",
    "        current_objects = {}\n",
    "        \n",
    "        for result in results:\n",
    "            boxes = result.boxes.cpu().numpy()\n",
    "            \n",
    "            for box in boxes:\n",
    "                # Get detection information\n",
    "                x1, y1, x2, y2 = box.xyxy[0].astype(int)\n",
    "                conf = box.conf[0]\n",
    "                class_id = int(box.cls[0])\n",
    "                class_name = self.model.names[class_id]\n",
    "                \n",
    "                if conf < self.conf_threshold:\n",
    "                    continue\n",
    "                    \n",
    "                # Check if object overlaps with any existing tracked object\n",
    "                matched = False\n",
    "                for track_id, tracked_obj in self.tracking_objects.items():\n",
    "                    tracked_box = tracked_obj['box']\n",
    "                    iou = self.calculate_iou([x1, y1, x2, y2], tracked_box)\n",
    "                    \n",
    "                    if iou > self.iou_threshold:\n",
    "                        current_objects[track_id] = {\n",
    "                            'box': [x1, y1, x2, y2],\n",
    "                            'class': class_name,\n",
    "                            'confidence': conf,\n",
    "                            'tracked_frames': tracked_obj['tracked_frames'] + 1\n",
    "                        }\n",
    "                        matched = True\n",
    "                        break\n",
    "                \n",
    "                # If no match found, create new tracked object\n",
    "                if not matched:\n",
    "                    self.track_id += 1\n",
    "                    current_objects[self.track_id] = {\n",
    "                        'box': [x1, y1, x2, y2],\n",
    "                        'class': class_name,\n",
    "                        'confidence': conf,\n",
    "                        'tracked_frames': 1\n",
    "                    }\n",
    "        \n",
    "        # Update tracking objects\n",
    "        self.tracking_objects = current_objects\n",
    "        \n",
    "        return self.tracking_objects\n",
    "\n",
    "    def draw_results(self, frame, objects):\n",
    "        \"\"\"\n",
    "        Draw detection and tracking results on frame\n",
    "        \"\"\"\n",
    "        for track_id, obj in objects.items():\n",
    "            x1, y1, x2, y2 = obj['box']\n",
    "            class_name = obj['class']\n",
    "            conf = obj['confidence']\n",
    "            tracked_frames = obj['tracked_frames']\n",
    "            \n",
    "            # Draw box with different colors based on tracking duration\n",
    "            color = (0, int(min(255, tracked_frames * 10)), 0)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            \n",
    "            # Draw label with tracking ID\n",
    "            label = f\"#{track_id} {class_name} {conf:.2f}\"\n",
    "            cv2.putText(frame, label, (x1, y1-10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "            \n",
    "        return frame\n",
    "\n",
    "    def run_detection(self, source=0, display=True):\n",
    "        \"\"\"\n",
    "        Run real-time detection and tracking\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(source)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Couldn't open video source\")\n",
    "            return\n",
    "        \n",
    "        print(\"Detection and tracking started... Press 'q' to quit\")\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                # Process frame\n",
    "                tracked_objects = self.process_frame(frame)\n",
    "                \n",
    "                # Draw results\n",
    "                if display:\n",
    "                    frame = self.draw_results(frame, tracked_objects)\n",
    "                    \n",
    "                    # Calculate and display FPS\n",
    "                    fps = 1.0 / (time.time() - start_time)\n",
    "                    self.fps_history.append(fps)\n",
    "                    if len(self.fps_history) > 30:\n",
    "                        self.fps_history.pop(0)\n",
    "                    avg_fps = sum(self.fps_history) / len(self.fps_history)\n",
    "                    \n",
    "                    cv2.putText(frame, f'FPS: {avg_fps:.1f}', (10, 30),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                    \n",
    "                    cv2.imshow('Object Detection and Tracking', frame)\n",
    "                \n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "        finally:\n",
    "            cap.release()\n",
    "            if display:\n",
    "                cv2.destroyAllWindows()\n",
    "\n",
    "def main():\n",
    "    # Initialize detector with quantized model\n",
    "    detector = ObjectDetectionTracker(\n",
    "        model_path='yolov8n_quantized.pt',  # Using the quantized model\n",
    "        confidence_threshold=0.5,\n",
    "        iou_threshold=0.3\n",
    "    )\n",
    "    \n",
    "    # Run detection\n",
    "    detector.run_detection()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a05f8b5-abbe-4910-aa80-13fe3daaacd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "class YOLOModelComparator:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize with default YOLO models\n",
    "        \"\"\"\n",
    "        # Dictionary to store model names and their corresponding YOLO models\n",
    "        self.models = {\n",
    "            'YOLOv8n': YOLO('yolov8n.pt'),\n",
    "            'YOLOv8s': YOLO('yolov8s.pt')\n",
    "        }\n",
    "        \n",
    "        # COCO class names for label mapping\n",
    "        self.class_names = [\n",
    "            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck',\n",
    "            'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench',\n",
    "            'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra',\n",
    "            'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "            'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "            'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
    "            'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n",
    "            'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "            'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse',\n",
    "            'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
    "            'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
    "            'toothbrush'\n",
    "        ]\n",
    "        \n",
    "    def detect_objects(self, image_path, conf_threshold=0.25):\n",
    "        \"\"\"\n",
    "        Perform object detection using all models\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Read the image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Could not read image at {image_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Run detection with each model\n",
    "        for model_name, model in self.models.items():\n",
    "            detections = model(image, conf=conf_threshold)[0]\n",
    "            \n",
    "            # Extract detection information\n",
    "            boxes = detections.boxes.xyxy.cpu().numpy()\n",
    "            confidences = detections.boxes.conf.cpu().numpy()\n",
    "            class_ids = detections.boxes.cls.cpu().numpy().astype(int)\n",
    "            \n",
    "            # Store results\n",
    "            results[model_name] = {\n",
    "                'boxes': boxes,\n",
    "                'confidences': confidences,\n",
    "                'class_ids': class_ids,\n",
    "                'labels': [self.class_names[id] for id in class_ids]\n",
    "            }\n",
    "            \n",
    "        return results, image\n",
    "\n",
    "    def visualize_detections(self, image_path, conf_threshold=0.25):\n",
    "        \"\"\"\n",
    "        Visualize detections from all models side by side\n",
    "        \"\"\"\n",
    "        # Get detections\n",
    "        try:\n",
    "            results, image = self.detect_objects(image_path, conf_threshold)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during detection: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Create figure\n",
    "        num_models = len(self.models)\n",
    "        fig, axes = plt.subplots(1, num_models, figsize=(15, 5))\n",
    "        if num_models == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        # Color map for different classes\n",
    "        colors = plt.cm.hsv(np.linspace(0, 1, 80)).tolist()\n",
    "\n",
    "        # Plot detections for each model\n",
    "        for ax, (model_name, detections) in zip(axes, results.items()):\n",
    "            ax.imshow(image)\n",
    "            \n",
    "            # Draw boxes and labels for each detection\n",
    "            for box, conf, label, class_id in zip(\n",
    "                detections['boxes'],\n",
    "                detections['confidences'],\n",
    "                detections['labels'],\n",
    "                detections['class_ids']\n",
    "            ):\n",
    "                x1, y1, x2, y2 = box.astype(int)\n",
    "                color = colors[class_id]\n",
    "                \n",
    "                # Draw box\n",
    "                rect = plt.Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "                                   fill=False,\n",
    "                                   color=color,\n",
    "                                   linewidth=2)\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                # Add label\n",
    "                label_text = f'{label} {conf:.2f}'\n",
    "                ax.text(x1, y1-10, label_text,\n",
    "                       color=color,\n",
    "                       fontsize=8,\n",
    "                       bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "            ax.set_title(f'{model_name}\\nDetections: {len(detections[\"boxes\"])}')\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def compare_performance(self, image_path, num_runs=5):\n",
    "        \"\"\"\n",
    "        Compare model performance metrics\n",
    "        \"\"\"\n",
    "        performance_metrics = {}\n",
    "        \n",
    "        for model_name, model in self.models.items():\n",
    "            # Measure inference time\n",
    "            times = []\n",
    "            for _ in range(num_runs):\n",
    "                start_time = time.time()\n",
    "                _ = model(image_path)\n",
    "                times.append(time.time() - start_time)\n",
    "            \n",
    "            # Get detections for object count\n",
    "            results = model(image_path)[0]\n",
    "            num_objects = len(results.boxes)\n",
    "            \n",
    "            performance_metrics[model_name] = {\n",
    "                'avg_inference_time': np.mean(times),\n",
    "                'std_inference_time': np.std(times),\n",
    "                'num_objects_detected': num_objects\n",
    "            }\n",
    "            \n",
    "        return performance_metrics\n",
    "\n",
    "def main():\n",
    "    # Create comparator instance\n",
    "    comparator = YOLOModelComparator()\n",
    "    \n",
    "    # Path to your test image\n",
    "    image_path = 'images/3892.jpg'  # Replace with your image path\n",
    "    \n",
    "    try:\n",
    "        # Visualize detections\n",
    "        fig = comparator.visualize_detections(image_path, conf_threshold=0.25)\n",
    "        if fig:\n",
    "            plt.show()\n",
    "        \n",
    "        # Compare performance\n",
    "        metrics = comparator.compare_performance(image_path)\n",
    "        \n",
    "        # Print performance metrics\n",
    "        print(\"\\nPerformance Metrics:\")\n",
    "        for model_name, metric in metrics.items():\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            print(f\"Average Inference Time: {metric['avg_inference_time']:.3f} seconds\")\n",
    "            print(f\"Standard Deviation: {metric['std_inference_time']:.3f} seconds\")\n",
    "            print(f\"Objects Detected: {metric['num_objects_detected']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during comparison: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c894cd3-a899-4ed4-ae87-08714ae200b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can clearlyy see yolov8s  >> yolov8n\n",
    "#now comparre with quantized one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90acba7f-2984-4dfa-b567-fbee56b2cffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "\n",
    "class QuantizedDetector:\n",
    "    def __init__(self, model_path):\n",
    "        \"\"\"\n",
    "        Initialize with quantized model path\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load the base model first\n",
    "            self.model = YOLO('yolov8n.pt')\n",
    "            \n",
    "            # Load the quantized state dict\n",
    "            state_dict = torch.load(model_path)\n",
    "            if 'state_dict' in state_dict:\n",
    "                self.model.model.load_state_dict(state_dict['state_dict'])\n",
    "            elif 'model' in state_dict:\n",
    "                self.model.model.load_state_dict(state_dict['model'])\n",
    "            else:\n",
    "                self.model.model.load_state_dict(state_dict)\n",
    "                \n",
    "            print(f\"Model loaded successfully from {model_path}\")\n",
    "            \n",
    "            # Move model to GPU if available\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            self.model.to(self.device)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            raise\n",
    "\n",
    "        # COCO class names\n",
    "        self.class_names = [\n",
    "            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck',\n",
    "            'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench',\n",
    "            'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra',\n",
    "            'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "            'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "            'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
    "            'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n",
    "            'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "            'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse',\n",
    "            'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
    "            'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
    "            'toothbrush'\n",
    "        ]\n",
    "\n",
    "    def detect_objects(self, image_path, conf_threshold=0.25):\n",
    "        \"\"\"\n",
    "        Detect objects in an image and return detailed results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Read image\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Could not read image at {image_path}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Time the inference\n",
    "            start_time = time.time()\n",
    "            results = self.model(image, conf=conf_threshold)[0]\n",
    "            inference_time = time.time() - start_time\n",
    "\n",
    "            # Extract detection information\n",
    "            boxes = results.boxes.xyxy.cpu().numpy()\n",
    "            confidences = results.boxes.conf.cpu().numpy()\n",
    "            class_ids = results.boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "            # Count objects by class\n",
    "            class_counts = {}\n",
    "            for class_id, conf in zip(class_ids, confidences):\n",
    "                class_name = self.class_names[class_id]\n",
    "                if class_name not in class_counts:\n",
    "                    class_counts[class_name] = {\n",
    "                        'count': 0,\n",
    "                        'confidences': []\n",
    "                    }\n",
    "                class_counts[class_name]['count'] += 1\n",
    "                class_counts[class_name]['confidences'].append(conf)\n",
    "\n",
    "            detection_results = {\n",
    "                'boxes': boxes,\n",
    "                'confidences': confidences,\n",
    "                'class_ids': class_ids,\n",
    "                'class_counts': class_counts,\n",
    "                'total_objects': len(boxes),\n",
    "                'inference_time': inference_time\n",
    "            }\n",
    "\n",
    "            return detection_results, image\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during detection: {e}\")\n",
    "            raise\n",
    "\n",
    "    def visualize_detections(self, image_path, conf_threshold=0.25):\n",
    "        \"\"\"\n",
    "        Visualize detected objects with bounding boxes and labels\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get detections\n",
    "            results, image = self.detect_objects(image_path, conf_threshold)\n",
    "\n",
    "            # Create figure\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.imshow(image)\n",
    "\n",
    "            # Color map for different classes\n",
    "            colors = plt.cm.hsv(np.linspace(0, 1, 80)).tolist()\n",
    "\n",
    "            # Draw boxes and labels\n",
    "            for box, conf, class_id in zip(results['boxes'], \n",
    "                                         results['confidences'], \n",
    "                                         results['class_ids']):\n",
    "                x1, y1, x2, y2 = box.astype(int)\n",
    "                label = self.class_names[class_id]\n",
    "                color = colors[class_id]\n",
    "\n",
    "                # Draw box\n",
    "                rect = plt.Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "                                   fill=False,\n",
    "                                   color=color,\n",
    "                                   linewidth=2)\n",
    "                plt.gca().add_patch(rect)\n",
    "\n",
    "                # Add label\n",
    "                label_text = f'{label} {conf:.2f}'\n",
    "                plt.text(x1, y1-10, label_text,\n",
    "                        color=color,\n",
    "                        fontsize=8,\n",
    "                        bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "            plt.title(f'Total Detections: {results[\"total_objects\"]}\\n'\n",
    "                     f'Inference Time: {results[\"inference_time\"]:.3f} seconds')\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "\n",
    "            return plt.gcf()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during visualization: {e}\")\n",
    "            raise\n",
    "\n",
    "    def print_detection_stats(self, image_path, conf_threshold=0.25):\n",
    "        \"\"\"\n",
    "        Print detailed statistics about detections\n",
    "        \"\"\"\n",
    "        try:\n",
    "            results, _ = self.detect_objects(image_path, conf_threshold)\n",
    "\n",
    "            print(\"\\nDetection Statistics:\")\n",
    "            print(\"=\" * 50)\n",
    "            print(f\"\\nTotal Objects Detected: {results['total_objects']}\")\n",
    "            print(f\"Inference Time: {results['inference_time']:.3f} seconds\")\n",
    "\n",
    "            print(\"\\nDetections by Class:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"{'Class':<20} {'Count':<10} {'Avg Confidence':<15}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            for class_name, data in results['class_counts'].items():\n",
    "                avg_conf = np.mean(data['confidences'])\n",
    "                print(f\"{class_name:<20} {data['count']:<10} {avg_conf:.3f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error printing statistics: {e}\")\n",
    "            raise\n",
    "\n",
    "def main():\n",
    "    # Initialize detector with your quantized model\n",
    "    model_path = \"yolov8n_quantized.pt\"  # Replace with your actual quantized model path\n",
    "    detector = QuantizedDetector(model_path)\n",
    "\n",
    "    # Path to test image\n",
    "    image_path = \"images/3892.jpg\"  # Replace with your image path\n",
    "\n",
    "    try:\n",
    "        # Print detection statistics\n",
    "        detector.print_detection_stats(image_path)\n",
    "\n",
    "        # Visualize detections\n",
    "        fig = detector.visualize_detections(image_path)\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca535dbe-d750-4017-a640-b1b569b5172c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9e4c32-4ba9-48f0-88e2-8cbac39e1e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5d1165-166f-43e8-b04f-e6ee9d1a29fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a01801-812b-45d6-807d-892974659aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40edd94-0b4b-43b2-8d8c-9773c3c48491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2dc4df-27cd-4865-88c9-b031e76ed2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
